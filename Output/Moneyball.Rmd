---
title: "Moneyball"
subtitle: "ADEC 7320.02"
author: "Silas Selfe"
affiliation: "Boston College"
date: "11/9/2021"
output: pdf_document
editor_options: 
  chunk_output_type: console
  markdown: 
    wrap: 72
---



```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(warning=FALSE, message=FALSE)

```

```{r, echo=FALSE}
library(rticles)
library(tinytex)
library(xtable)
library(stargazer)
library(pander)
library(tables)
library(ascii)
library(knitr)
library(magrittr)
library(dplyr)
library(fastGraph)
library(gridExtra)
library(psych)
library(png)
library(kableExtra)
library(cowplot)
library(float)
library(MASS)

```


```{r echo=FALSE, message=FALSE}
library(readr)
library(psych)
library(xtable)
library(Amelia)
library(ggplot2)


setwd("C:/Users/Silas/Documents/ADEC7320")
dataEval <- read_csv("Data/moneyball-evaluation-data.csv")
data <- read_csv("Data/moneyball-training-data.csv")
show_col_types = FALSE
```





\newpage

# Data Exploration

```{r echo=FALSE, message=FALSE}
data = data %>%
    rename(
      Index = INDEX,
      numWins = TARGET_WINS,
      baseHits = TEAM_BATTING_H,
      doubles = TEAM_BATTING_2B,
      triples = TEAM_BATTING_3B,
      homeruns = TEAM_BATTING_HR,
      walksTaken = TEAM_BATTING_BB,
      freeBase = TEAM_BATTING_HBP,
      strikeOuts = TEAM_BATTING_SO,
      stolenBases = TEAM_BASERUN_SB,
      caughtStealing = TEAM_BASERUN_CS,
      errors = TEAM_FIELDING_E,
      doublePlays = TEAM_FIELDING_DP,
      walksAllowed = TEAM_PITCHING_BB,
      hitsAllowed = TEAM_PITCHING_H,
      homerunsAllowed = TEAM_PITCHING_HR,
      strikeOuts_byPitchers = TEAM_PITCHING_SO
    )



dataNames = colnames(data)
posCor = matrix(nrow = 6, ncol = 4)
negCor = matrix(nrow = 4, ncol = 4)

i = 1
r = 1
cH = 1    # count Hold
rowC = 0
count = 0
countTotal = 0
for (i in 2:(ncol(data)-1)){
  for (r in (i+1):ncol(data)){
    if(as.double(cor(data[,i], data[,r], method = "pearson", use = "complete.obs")) > 0.5){
      count = count + 1
      countTotal = countTotal + 1
      
      if(count == 1){
        rowC = rowC + 1
        posCor[rowC,count] = dataNames[i]
      }
      posCor[rowC,count+1] = dataNames[r]
    }
    r = r + 1
  }
  
  if(cH <= count){
    cH = cH + 1
  }
  count = 0
  i = i + 1
}


i = 1
r = 1
cH = 1    # count Hold
rowC = 0
countTotal = 0
for (i in 2:(ncol(data)-1)){
  for (r in (i+1):ncol(data)){
    if(as.double(cor(data[,i], data[,r], method = "pearson", use = "complete.obs")) < -0.5){
      count = count + 1
      countTotal = countTotal + 1
      
      if(count == 1){
        rowC = rowC + 1
        negCor[rowC,count] = dataNames[i]
      }
      negCor[rowC,count+1] = dataNames[r]
    }
    r = r + 1
  }
  
  if(cH <= count){
    cH = cH + 1
  }
  count = 0
  i = i + 1
}

```



```{r echo=FALSE, message =FALSE}
# PLOTS

# Positive Correlation
pos1 <- ggplot(data = data, mapping = aes(x = baseHits, y = doubles)) + 
  geom_point(color = "blue4") + 
  geom_smooth(color = "green1")

pos2 <- ggplot(data = data, mapping = aes(x = triples, y = stolenBases)) + 
  geom_point(color = "steelblue") + 
  geom_smooth(color = "purple4")
pos3 <- ggplot(data = data, mapping = aes(x = triples, y = errors)) + 
  geom_point(color = "steelblue") + 
  geom_smooth(color = "blue1")

pos4 <- ggplot(data = data, mapping = aes(x = homeruns, y = walksTaken)) + 
  geom_point(color = "red1") + 
  geom_smooth(color = "magenta1")
pos5 <- ggplot(data = data, mapping = aes(x = homeruns, y = strikeOuts)) + 
  geom_point(color = "red1") + 
  geom_smooth(color = "orange1")
pos6 <- ggplot(data = data, mapping = aes(x = homeruns, y = homerunsAllowed)) + 
  geom_point(color = "red1") + 
  geom_smooth(color = "yellow2")

pos7 <- ggplot(data = data, mapping = aes(x = strikeOuts, y = homerunsAllowed)) + 
  geom_point(color = "orange1") + 
  geom_smooth(color = "yellow2")

pos8 <- ggplot(data = data, mapping = aes(x = stolenBases, y = caughtStealing)) + 
  geom_point(color = "purple4") + 
  geom_smooth(color = "cyan1")
pos9 <- ggplot(data = data, mapping = aes(x = stolenBases, y = errors)) + 
  geom_point(color = "purple4") + 
  geom_smooth(color = "blue1")

pos10 <- ggplot(data = data, mapping = aes(x = hitsAllowed, y = errors)) + 
  geom_point(color = "aquamarine4") + 
  geom_smooth(color = "blue1")



```



```{r echo=FALSE, message=FALSE, include=FALSE}
img1_path <- "/Users/Silas/Documents/ADEC7320/Output/Rplot.png"
img1 <- readPNG(img1_path, native = TRUE, info = TRUE)
attr(img1, "info")

img2_path <- "/Users/Silas/Documents/ADEC7320/Output/pg2lower.png"
img2 <- readPNG(img2_path, native = TRUE, info = TRUE)
attr(img2, "info")

img3_path <- "/Users/Silas/Documents/ADEC7320/Output/pg5_final.png"
img3 <- readPNG(img3_path, native = TRUE, info = TRUE)
attr(img3, "info")

img4_path <- "/Users/Silas/Documents/ADEC7320/Output/pg4plot.png"
img4 <- readPNG(img4_path, native = TRUE, info = TRUE)
attr(img4, "info")
```


```{r echo=FALSE, dpi = 210, fig.align='center'}
include_graphics(img1_path)

```

```{r echo=FALSE, dpi = 155, fig.align='center'}
include_graphics(img2_path)

```



\newpage

```{r out.height="200%", echo=FALSE, message=FALSE, results = 'asis'}
kable(posCor[,], col.names = c("Compared Variable", "V1", "V2", "V3"), caption="Variables with Positive Correlation ( > 0.50)", booktabs = T) %>%
  kable_styling(latex_options = c("striped", "hold_position"))

```
```{r echo=FALSE, dpi = 140, fig.align='center'}
include_graphics(img4_path)

```

&nbsp;  

The variables with positive correlation greater than 0.5 seem to follow their general distribution patterns seen in the variable plots on page 2. The variable that most apparently disrupts these patterns is errors, which is seen immediately by the outliers it produces in plots [1,3] and [1,4].  

While some plots are somewhat frivolous, the ones that are not raise interesting questions. [1,3] Does having a batter on third result in a pressured outfield making more errors? [1,2] If a pitcher has a faster pitch, does this result in more Strike Outs? If so, when batters hit a faster pitch, does it translate into more Home Runs? And most important is the question being answered, how can this data predict the number of wins for a given team?

\newpage

```{r out.height="50%", echo=FALSE, message=FALSE}
kable(negCor[,], col.names = c("Compared Variable", "V1", "V2", "v3"), caption="Variables with Negative Correlation ( < 0.50)", booktabs = T) %>%
  kable_styling(latex_options = c("striped", "hold_position"))

# Negative Correlation
neg1 <- ggplot(data = data, mapping = aes(x = triples, y = homeruns)) + 
  geom_point(color = "steelblue") + 
  geom_smooth(color = "red1")
neg2 <- ggplot(data = data, mapping = aes(x = triples, y = strikeOuts)) + 
  geom_point(color = "steelblue") + 
  geom_smooth(color = "orange1")
neg3 <- ggplot(data = data, mapping = aes(x = triples, y = homerunsAllowed)) + 
  geom_point(color = "steelblue") + 
  geom_smooth(color = "yellow2")

neg4 <- ggplot(data = data, mapping = aes(x = homeruns, y = errors)) + 
  geom_point(color = "red1") + 
  geom_smooth(color = "blue1")

neg5 <- ggplot(data = data, mapping = aes(x = walksTaken, y = errors)) + 
  geom_point(color = "magenta1") + 
  geom_smooth(color = "blue1")

neg6 <- ggplot(data = data, mapping = aes(x = strikeOuts, y = errors)) + 
  geom_point(color = "orange1") + 
  geom_smooth(color = "blue1")

grid.arrange(neg1, neg4, neg2, neg5, neg3, neg6, nrow = 3)
```

&nbsp;  

What is interesting in these negatively correlated variables is that "triples" is correlated with the only three variables that take a binomial distribution. It could follow a similar sort of logic discussed on page 3. Plot [2,1] shows that there are fewer triples when there are more strikeouts, so a fast throwing pitcher is bound to get more strike outs, but batters who make contact with this pitch are more likely to hit deep into the outfield. This hypotheses seems to hold weight by looking at [1,1] where a similar correlation is seen which suggests that a faster pitch results in more strikeouts, while also allowing further hits, which result in homeruns and triples.   

The "errors" variable also negatively contributes to the variables seen on the right. It is possible that in [1,2], if there are fewer homeruns, there is a slower pitcher that allows more batters to hit into the infield, creating opportunity for errors to arise. In the same spirit, in [2,2], if more batters are walked, the ball is not frequently in play which reduces the opportunity for errors. The same logic is followed with [3,2] as a strikeout almost entirely removes the opportunity for error. 



\newpage
```{r echo=FALSE, message=FALSE}
# Manipulation

boxData <- data[,c(3,12,16)]
```



## Batting v Pitching v Game Variables
```{r echo=FALSE, dpi = 130, fig.align='center'}
include_graphics(img3_path)

summary(boxData)

```

&nbsp;  

These comparisons illustrate what appear to be key determinants in the number of games a team wins. It should come as no surprise that winning and losing seemingly come down to a teams ability to hit the ball and how well they are able to prevent their opponent from doing so. 





\newpage




# Data Preparation



```{r echo=FALSE, message=FALSE}
missmap(data)
```

&nbsp;  

Since the variable "***freebase***" shows no significant correlation with other variables, it will be removed from the modeling dataset. 
The missing values for the variables "***double plays***" and "***stolen bases***" are replaced by their corresponding mean value since they follow a normal distribution and similarly, for "***strikeouts***" and "***strikeouts by pitcher***" they're replaced by their median values as their distributions aren't neatly normal. 

For the "***caught stealing***" variable, its missing values--determined by a dataset with all NAs removed--are replaced by the observed value of "***stolen bases***" multiplied by 0.6115212, which was found to be the mean proportion of getting caught stealing. 


```{r echo=FALSE, message=FALSE}
mdlData <- data[, c(2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17)]

#mdlData$caughtStealing[is.na(mdlData$caughtStealing)] <- mean(mdlData$caughtStealing, na.rm = TRUE)

mdlData$doublePlays[is.na(mdlData$doublePlays)] <- mean(mdlData$doublePlays, na.rm = TRUE)
mdlData$stolenBases[is.na(mdlData$stolenBases)] <- mean(mdlData$stolenBases, na.rm = TRUE)
mdlData$caughtStealing[is.na(mdlData$caughtStealing)] <- mdlData$stolenBases * 0.6115212
mdlData$freeBase[is.na(mdlData$freeBase)] <- runif(sum(is.na(mdlData$freeBase)), min = 29, max = 95)
mdlData$strikeOuts[is.na(mdlData$strikeOuts)] <- median(mdlData$strikeOuts, na.rm = TRUE)
mdlData$strikeOuts_byPitchers[is.na(mdlData$strikeOuts_byPitchers)] <- median(mdlData$strikeOuts_byPitchers, na.rm = TRUE)


mdlData <- filter(mdlData, strikeOuts != 0)
mdlData <- filter(mdlData, triples != 0)

# Total Hits
mdlData$totHits <- mdlData$baseHits + mdlData$doubles + mdlData$triples + mdlData$homeruns

# Change between batting v fielding
mdlData$bat_field <- mdlData$strikeOuts / 3
# Average hits per inning
mdlData$hits_inning <-  mdlData$totHits / mdlData$bat_field 

# Hits Taken v Hits Allowed
mdlData$hit_prob <- mdlData$totHits / mdlData$hitsAllowed
mdlData$hit_diff <- mdlData$totHits - mdlData$hitsAllowed

# Probability that a hit is a home run
mdlData$homer_prob <- mdlData$homeruns / mdlData$totHits
mdlData$homer_diff <- mdlData$homeruns - mdlData$homerunsAllowed

# Probability that an allowed hit will result in an error
mdlData$error_prob <- mdlData$errors / mdlData$hitsAllowed


```

&nbsp;  


New variables were also created. Among them are the probability of a hit being a home run, the proportion in which a team hits compared to the amount of hits they allow, the proportion of hits per error, an estimated amount of times a team would change from batting to fielding in a season, etc. etc. 

Much time was spent here. An attempt to calculate the On Base Percentage was made, as well as the Slugging Percentage. The variables were manipulated heavily in an attempt to minimize the squared error and tighten the models fit. The most useful variable that was created was the "***totHits***" variable that simply summed up all hits for a team. As is seen in the models below, not many created variables contributed in a significant way to any of the models. 


\newpage  


# Models

## Model 1
```{r echo = FALSE, message = FALSE}
mylm <- lm(numWins ~ totHits + strikeOuts + 
             hitsAllowed + hit_prob + errors + 
             error_prob + stolenBases + homer_prob, data = mdlData)

summary(mylm)


setwd("C:/Users/Silas/Documents/ADEC7320")
testPrep <- read.csv("Data/moneyball-evaluation-data.csv", stringsAsFactors = T)

testPrep = testPrep %>%
  rename(
    Index = INDEX,
    baseHits = TEAM_BATTING_H,
    doubles = TEAM_BATTING_2B,
    triples = TEAM_BATTING_3B,
    homeruns = TEAM_BATTING_HR,
    walksTaken = TEAM_BATTING_BB,
    freeBase = TEAM_BATTING_HBP,
    strikeOuts = TEAM_BATTING_SO,
    stolenBases = TEAM_BASERUN_SB,
    caughtStealing = TEAM_BASERUN_CS,
    errors = TEAM_FIELDING_E,
    doublePlays = TEAM_FIELDING_DP,
    walksAllowed = TEAM_PITCHING_BB,
    hitsAllowed = TEAM_PITCHING_H,
    homerunsAllowed = TEAM_PITCHING_HR,
    strikeOuts_byPitchers = TEAM_PITCHING_SO
  )

testPrep$strikeOuts[is.na(testPrep$strikeOuts)] <- median(testPrep$strikeOuts, na.rm=TRUE)
testPrep$stolenBases[is.na(testPrep$stolenBases)] <- mean(testPrep$stolenBases, na.rm = TRUE)

# Total Hits
testPrep$totHits <- testPrep$baseHits + testPrep$doubles + testPrep$triples + testPrep$homeruns

# Change between batting v fielding
testPrep$bat_field <- testPrep$strikeOuts / 3
# Average hits per inning
testPrep$hits_inning <-  testPrep$totHits / testPrep$bat_field 

# Hits Taken v Hits Allowed
testPrep$hit_prob <- testPrep$totHits / testPrep$hitsAllowed
testPrep$hit_diff <- testPrep$totHits - testPrep$hitsAllowed

# Probability that a hit is a home run
testPrep$homer_prob <- testPrep$homeruns / testPrep$totHits
testPrep$homer_diff <- testPrep$homeruns - testPrep$homerunsAllowed

# Probability that an allowed hit will result in an error
testPrep$error_prob <- testPrep$errors / testPrep$hitsAllowed


teamINDEX <- testPrep[,1]
mypred1 <- round(predict(mylm, testPrep))

test <- testPrep[,1] 

mydf <- data.frame(teamINDEX, mypred1)

```

&nbsp;   


The variables in this model were chosen with the intent of keeping each variably significant to its prediction. The coefficients of these variables all make sense. To clarify, the variable ***hit_prob*** depicts how many hits a team has for every hit of their opponent. 

\newpage  

## Model 2

```{r echo = FALSE, message = FALSE}
mylm <- lm(numWins ~ totHits + baseHits + doubles + triples + 
             walksTaken + strikeOuts + stolenBases + hitsAllowed + 
             homerunsAllowed + walksAllowed + errors + homer_prob + error_prob, data = mdlData)
summary(mylm)

mypred2 <- round(predict(mylm, testPrep))

mydf <- data.frame(mydf, mypred2)
```

&nbsp;  

This model includes more variables in an attempt to make its predictions a closer fit to the actual values. While its R_squared value increases slightly, its variables lose their significance. Many of its coefficients also do not make sense, yet where they don't, they're close to zero. This may be caused by the model attempting to overfit because some of its variables count the same information more than once. This model will not be kept. 


\newpage  


## Model 3

```{r echo=FALSE, message=FALSE}
mylm <- lm(numWins ~ baseHits + doubles + triples + homeruns + walksTaken + strikeOuts + stolenBases + 
             hitsAllowed + homerunsAllowed + walksAllowed + errors +
             hits_inning + hit_prob + homer_prob + error_prob, data = mdlData)
summary(mylm)

mypred3 <- round(predict(mylm, testPrep))
mydf <- data.frame(mydf, mypred3)

```

&nbsp;  

This model attempts to remove the overfitting by reducing the amount of times a variable and its transformed values are included. The coefficients on this model make sense, however not all variables contribute significantly to the overall model. There are variables that don't contribute, but the model has a tighter fit to the actual values which is seen by its R_squared value. 


\newpage  

# Select Models

The model to be selected is Model 1. This is chosen in sacrifice of the slightly better R_squared value seen in Model 3 because all of its variables contribute significantly to the model, reducing the opportunity for multi-collinearity to arise, and also because it has a larger F-statistic. This reinforces the significance of the variables used by showing their joint effect on the model. As seen in the predictions of the three models, all predict similar values, however with such a loosely fitted model there is plenty of room for improvement in its overall accuracy. Seen below are the first 25 predicted values from all three models. 

```{r echo = FALSE}
mydf[c(1:25),]
```



























